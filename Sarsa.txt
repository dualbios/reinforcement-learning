Cliff walking

—осто€ни€ = позиции
ƒействи€ = направлени€
Ќаграды = -1 за шаг, -100 (и конец) за падение, конец за достижение точки победы

≈сть матрица ценностей Q(S, A), которую мы пытаемс€ расчитать
≈сть исследовательска€ политика:
  e   -> случайное направление
  1-e -> жадное?

TD:
  v(s) = (1-a)*v(s) + a*(R + g*v(s'))
Sarsa (on policy):
  Q(s,a) = (1-a)*Q(s,a) + a*(R + g*Q(s',a'))
Q-learning (off policy):
  Q(s,a) = (1-a)*Q(s,a) + a*(R + g*max_a' Q(s',a'))
